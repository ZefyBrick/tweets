{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc7dc83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from termcolor import colored\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e3eb43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Считывание данных\n",
    "negative = pd.read_csv('processedNegative.csv')\n",
    "neutral = pd.read_csv('processedNeutral.csv')\n",
    "positive = pd.read_csv('processedPositive.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c2420c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Преобразование\n",
    "df_1 = pd.DataFrame({'tweets': negative.columns, 'type': 'negative'})\n",
    "df_2 = pd.DataFrame({'tweets': neutral.columns, 'type': 'neutral'})\n",
    "df_3 = pd.DataFrame({'tweets': positive.columns, 'type': 'positive'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b553555",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Объединение фреймов\n",
    "df = pd.concat([df_1, df_2, df_3]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ad24316",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Удаление дубликатов(так как мы имели дело с заголовками, то к повторяющимся добавились цифры в конце)\n",
    "for i, j in enumerate(df.tweets):\n",
    "    df.tweets.iloc[i] = df.tweets.iloc[i].lower()\n",
    "    df.tweets.iloc[i] = re.sub('[^a-zA-Z]', ' ', df.tweets.iloc[i])\n",
    "    df.tweets.iloc[i] = re.sub(r'\\s+', ' ', df.tweets.iloc[i])\n",
    "    df.tweets.iloc[i] = df.tweets.iloc[i].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3e7e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Разбиваем на тренировочную и тестовую группу\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['tweets'], df['type'], \n",
    "                                                    test_size=0.2, random_state=21, stratify=df['type'])\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "y_train.index = np.arange(len(y_train))\n",
    "y_test.index = np.arange(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec5a1dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Удаляем дублирующиеся твиты\n",
    "X_train.drop_duplicates(inplace=True, keep='first')\n",
    "y_train = y_train[y_train.index.isin(X_train.index)]\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55327608",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Находим индексы твитов, состоящих из 1 слова\n",
    "one_word = []\n",
    "for i, row in enumerate(X_train):\n",
    "    if len(row.split()) <= 1:\n",
    "        one_word.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f12ffef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Удаляем твиты из 1 слова, так как они не несут информации\n",
    "X_train = X_train[~X_train.index.isin(one_word)]\n",
    "y_train = y_train[y_train.index.isin(X_train.index)]\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8780ca8",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e6bcb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preparation_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "847687a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preparation_data_test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c7e081",
   "metadata": {},
   "source": [
    "#### Токенизация по словам - разделение предложений на слова компоненты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f674d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = []\n",
    "for i in range(X_train.shape[0]):\n",
    "    token.append(' '.join(nltk.word_tokenize(X_train[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b732048",
   "metadata": {},
   "outputs": [],
   "source": [
    "preparation_data['token'] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3df020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    token.append(' '.join(nltk.word_tokenize(X_test[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3a1d3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preparation_data_test['token'] = token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5260d7e",
   "metadata": {},
   "source": [
    "#### Лемматизация - приведение слов к канонической форме\n",
    "#### Стемминг - отрезание \"лишнего\" от корня слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b2be802",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = WordNetLemmatizer()\n",
    "st = PorterStemmer()\n",
    "lemma = []\n",
    "stemmer = []\n",
    "for i in range(X_train.shape[0]):\n",
    "    tweet = X_train[i].split()\n",
    "    stemmer.append([])\n",
    "    lemma.append([])\n",
    "    for j in tweet:\n",
    "        stemmer[i].append(st.stem(word=j))\n",
    "        lemma[i].append(lm.lemmatize(word=j))\n",
    "    stemmer[i] = ' '.join(stemmer[i])\n",
    "    lemma[i] = ' '.join(lemma[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ae05337",
   "metadata": {},
   "outputs": [],
   "source": [
    "preparation_data['stemmer'] = stemmer\n",
    "preparation_data['lemma'] = lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bef58695",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = WordNetLemmatizer()\n",
    "st = PorterStemmer()\n",
    "lemma = []\n",
    "stemmer = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    tweet = X_test[i].split()\n",
    "    stemmer.append([])\n",
    "    lemma.append([])\n",
    "    for j in tweet:\n",
    "        stemmer[i].append(st.stem(word=j))\n",
    "        lemma[i].append(lm.lemmatize(word=j))\n",
    "    stemmer[i] = ' '.join(stemmer[i])\n",
    "    lemma[i] = ' '.join(lemma[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87ae56de",
   "metadata": {},
   "outputs": [],
   "source": [
    "preparation_data_test['stemmer'] = stemmer\n",
    "preparation_data_test['lemma'] = lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9754c9",
   "metadata": {},
   "source": [
    "#### Исправление ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa793ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nomiss = [0] * X_train.shape[0]\n",
    "for i in range(X_train.shape[0]):\n",
    "    textBlb = TextBlob(X_train[i])\n",
    "    nomiss[i] = ''.join(textBlb.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a608479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preparation_data['nomiss'] = nomiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a46250dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nomiss = [0] * X_test.shape[0]\n",
    "for i in range(X_test.shape[0]):\n",
    "    textBlb = TextBlob(X_test[i])\n",
    "    nomiss[i] = ''.join(textBlb.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cee2c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "preparation_data_test['nomiss'] = nomiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baef8e0",
   "metadata": {},
   "source": [
    "#### Лемматизация и стемминг \"безошибочных\" датафреймов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c342f0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = WordNetLemmatizer()\n",
    "st = PorterStemmer()\n",
    "lemma = []\n",
    "stemmer = []\n",
    "for i in range(X_train.shape[0]):\n",
    "    tweet = preparation_data['nomiss'][i]\n",
    "    stemmer.append([])\n",
    "    lemma.append([])\n",
    "    for j in tweet:\n",
    "        stemmer[i].append(st.stem(word=j))\n",
    "        lemma[i].append(lm.lemmatize(word=j))\n",
    "    stemmer[i] = ''.join(stemmer[i])\n",
    "    lemma[i] = ''.join(lemma[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69e53f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "preparation_data['miss_stemmer'] = stemmer\n",
    "preparation_data['miss_lemma'] = lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6914c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = WordNetLemmatizer()\n",
    "st = PorterStemmer()\n",
    "lemma = []\n",
    "stemmer = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    tweet = preparation_data_test['nomiss'][i]\n",
    "    stemmer.append([])\n",
    "    lemma.append([])\n",
    "    for j in tweet:\n",
    "        stemmer[i].append(st.stem(word=j))\n",
    "        lemma[i].append(lm.lemmatize(word=j))\n",
    "    stemmer[i] = ''.join(stemmer[i])\n",
    "    lemma[i] = ''.join(lemma[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63da5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preparation_data_test['miss_stemmer'] = stemmer\n",
    "preparation_data_test['miss_lemma'] = lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebc74f5",
   "metadata": {},
   "source": [
    "## Создаём функции для работы с датафреймами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7caae12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## блок функций, позволяющий находить 10 похожих пар с помощью k-means и косинусного расстояния\n",
    "def pair_kmeans(data):\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0).fit(data)\n",
    "    result =  kmeans.labels_\n",
    "    x = [i for i, ltr in enumerate(result) if ltr == 0]\n",
    "    y = [i for i, ltr in enumerate(result) if ltr == 1]\n",
    "    ln_x = len(x)\n",
    "    ln_y = len(y)\n",
    "    if ln_y < ln_x:\n",
    "        if ln_y > 1:\n",
    "            return data[data.index.isin(y)]\n",
    "        else:\n",
    "            return data[data.index.isin(x)]\n",
    "    if ln_x <= ln_y:\n",
    "        if ln_x > 1:\n",
    "            return data[data.index.isin(x)]\n",
    "        else:\n",
    "            return data[data.index.isin(y)]\n",
    "\n",
    "        \n",
    "def drop_reset(data, names, indexes):\n",
    "    names.drop(indexes, inplace = True, axis = 0)\n",
    "    names.reset_index(drop=True, inplace=True)\n",
    "    data.drop(indexes, inplace = True, axis = 0)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "def find_ten_pairs(data, names):\n",
    "    i = 0\n",
    "    while i < 10:\n",
    "        indexes = [x for x in data.index]\n",
    "        df = pair_kmeans(data)\n",
    "        while df.shape[0] > 2:\n",
    "            indexes = [i for j, i in enumerate(indexes) if j in df.index]\n",
    "            df.reset_index(inplace=True, drop=True)\n",
    "            df = pair_kmeans(df)\n",
    "        indexes = [i for j, i in enumerate(indexes) if j in df.index]\n",
    "        if cosine_similarity([data.iloc[indexes[0]]], [data.iloc[indexes[1]]]) >= 0.8:\n",
    "            i += 1\n",
    "            print(f'{colored(i, \"red\", attrs=[\"bold\"])} {names.iloc[indexes[0]]}\\n  {names.iloc[indexes[1]]}', end='\\n\\n')\n",
    "        drop_reset(data, names, indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75bc831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Блок функций для создания фреймов с помощью word2vec\n",
    "def create_dictionary(data):\n",
    "    all_words = [x.split() for x in data]\n",
    "    for i in range(len(all_words)):\n",
    "        all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]\n",
    "    model = Word2Vec(all_words, min_count=4, vector_size=100, workers=3, window=2, sg=1)\n",
    "    vocabulary = model.wv\n",
    "    return model, vocabulary\n",
    "\n",
    "\n",
    "def data_to_vectors(data, size, model, vocabulary):\n",
    "    vectors = []\n",
    "    for i, row in enumerate(data):\n",
    "        vector = np.zeros(size).reshape((1, size))\n",
    "        for word in row.split():\n",
    "            try:\n",
    "                vector += vocabulary[model.wv.key_to_index[word]].reshape((1, size))\n",
    "            except KeyError:\n",
    "                continue\n",
    "        vectors.append(vector)\n",
    "    vectors = scale(np.concatenate([x for x in vectors]))\n",
    "    return pd.DataFrame(vectors)\n",
    "\n",
    "\n",
    "def create_vectors(data, data_test, size):\n",
    "    model, vocabulary = create_dictionary(data)\n",
    "    train = data_to_vectors(data, size, model, vocabulary)\n",
    "    test = data_to_vectors(data_test, size, model, vocabulary)\n",
    "    train.drop_duplicates(keep='first', inplace=True)\n",
    "    y_y_train = y_train[y_train.index.isin(train.index)]\n",
    "    x_train = X_train[X_train.index.isin(train.index)]\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    y_y_train.reset_index(drop=True, inplace=True)\n",
    "    x_train.reset_index(drop=True, inplace=True)\n",
    "    return train, test, x_train, y_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b2e5981",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Функция создания датафреймов для каждого вида обработки данных\n",
    "def to_bag(train_data, test_data, vectorizer):\n",
    "    train = train_data.to_list()\n",
    "    test = test_data.to_list()\n",
    "    bag_of_words = vectorizer.fit_transform(train)\n",
    "    bag_of_words_test = vectorizer.transform(test)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    df_bag = pd.DataFrame(bag_of_words.toarray(), columns = feature_names)\n",
    "    df_bag_test = pd.DataFrame(bag_of_words_test.toarray(), columns = feature_names)\n",
    "    df_bag.drop_duplicates(keep='first', inplace = True)\n",
    "    X_train_for_bag = X_train[X_train.index.isin(df_bag.index)]\n",
    "    y_train_for_bag = y_train[y_train.index.isin(df_bag.index)]\n",
    "    df_bag.reset_index(drop=True, inplace=True)\n",
    "    X_train_for_bag.reset_index(drop=True, inplace=True)\n",
    "    y_train_for_bag.reset_index(drop=True, inplace=True)\n",
    "    return df_bag, df_bag_test, X_train_for_bag, y_train_for_bag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe6ab0d",
   "metadata": {},
   "source": [
    "## Создаём мешки слов со стоп-словами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3aff6404",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "count_vectorizer = CountVectorizer(stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e485588a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m1\u001b[0m thanks for the recent follow happy to connect happy have a great wednesday\n",
      "  thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "\n",
      "\u001b[1m\u001b[31m2\u001b[0m i miss him unhappy\n",
      "  where s justin i miss him unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m3\u001b[0m hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "  hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "\u001b[1m\u001b[31m4\u001b[0m i love them with all my hort unhappy\n",
      "  love it unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m5\u001b[0m hey thanks for being top new followers this week much appreciated happy want this\n",
      "  hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m6\u001b[0m have a great thursday looking forward to reading your tweets happy want this\n",
      "  have a great thursday looking forward to reading your tweets happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m7\u001b[0m share the love thanks for being top new followers this week happy get it\n",
      "  share the love thanks for being top new followers this week happy\n",
      "\n",
      "\u001b[1m\u001b[31m8\u001b[0m rt good evening everyone happy join our twitter party tonight official tagline tanner welcomebackph tmi\n",
      "  rt almightytanner good evening everyone happy join our twitter party tonight official tagline tanner welcomeb\n",
      "\n",
      "\u001b[1m\u001b[31m9\u001b[0m thanks for being top engaged community members this week happy want this\n",
      "  thanks for being top engaged community members this week happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m10\u001b[0m stats for the day have arrived to new followers and no unfollowers happy via\n",
      "  stats for the day have arrived new follower and no unfollowers happy via\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Датафреймы для токенов в мешках слов и нахождение 10 пар похожих твитов\n",
    "df_token_bag, df_token_bag_test, X_train_for_token_bag, y_train_for_token_bag = to_bag(preparation_data['token'], \n",
    "                                                                                       preparation_data_test['token'], count_vectorizer)\n",
    "find_ten_pairs(df_token_bag.copy(), X_train_for_token_bag.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae1f696e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m1\u001b[0m hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "  hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "\u001b[1m\u001b[31m2\u001b[0m thanks for being top engaged community members this week happy want this\n",
      "  thanks for being top engaged community members this week happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m3\u001b[0m i miss him unhappy\n",
      "  where s justin i miss him unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m4\u001b[0m thanks for being top engaged community members this week happy try this too\n",
      "  thanks for being top engaged community members this week happy i sent this with\n",
      "\n",
      "\u001b[1m\u001b[31m5\u001b[0m thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "  thanks for the recent follow happy to connect happy have a great thursday want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m6\u001b[0m have a great thursday looking forward to reading your tweets happy want this\n",
      "  have a great thursday looking forward to reading your tweets happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m7\u001b[0m thanks for the recent follow happy to connect happy have a great wednesday\n",
      "  thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "\n",
      "\u001b[1m\u001b[31m8\u001b[0m thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "  thanks for the recent follow happy to connect happy have a great thursday\n",
      "\n",
      "\u001b[1m\u001b[31m9\u001b[0m hey thanks for being top new followers this week much appreciated happy want this\n",
      "  hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m10\u001b[0m share the love thanks for being top new followers this week happy get it\n",
      "  share the love thanks for being top new followers this week happy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Датафреймы для стеммера в мешках слов и нахождение 10 пар похожих твитов\n",
    "df_stemmer_bag, df_stemmer_bag_test, X_train_for_stemmer_bag, y_train_for_stemmer_bag = to_bag(preparation_data['stemmer'], \n",
    "                                                                                               preparation_data_test['stemmer'], count_vectorizer)\n",
    "find_ten_pairs(df_stemmer_bag.copy(), X_train_for_stemmer_bag.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2ba4b39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m1\u001b[0m thanks for the recent follow much appreciated happy want this\n",
      "  thanks for the recent follow much appreciated happy want this for it s magical\n",
      "\n",
      "\u001b[1m\u001b[31m2\u001b[0m hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "  hi ashish we tried to call your number but got no response unhappy please share another suitable time and an alternate cont\n",
      "\n",
      "\u001b[1m\u001b[31m3\u001b[0m i miss him unhappy\n",
      "  where s justin i miss him unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m4\u001b[0m have a great thursday looking forward to reading your tweets happy want this\n",
      "  have a great thursday looking forward to reading your tweets happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m5\u001b[0m share the love thanks for being top new followers this week happy get it\n",
      "  share the love thanks for being top new followers this week happy\n",
      "\n",
      "\u001b[1m\u001b[31m6\u001b[0m thanks for being top engaged community members this week happy want this\n",
      "  thanks for being top engaged community members this week happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m7\u001b[0m thanks for the recent follow happy to connect happy have a great wednesday\n",
      "  thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "\n",
      "\u001b[1m\u001b[31m8\u001b[0m thanks for the recent follow much appreciated happy i sent this with\n",
      "  thanks for the recent follow much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m9\u001b[0m thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "  thanks for the recent follow happy to connect happy have a great thursday want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m10\u001b[0m thanks for the recent follow happy to connect happy have a great thursday get free\n",
      "  thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Датафреймы для леммы в мешках слов и нахождение 10 пар похожих твитов\n",
    "df_lemma_bag, df_lemma_bag_test, X_train_for_lemma_bag, y_train_for_lemma_bag = to_bag(preparation_data['lemma'], \n",
    "                                                                                       preparation_data_test['lemma'], count_vectorizer)\n",
    "find_ten_pairs(df_lemma_bag.copy(), X_train_for_lemma_bag.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b44c041e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m1\u001b[0m hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "  hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "\u001b[1m\u001b[31m2\u001b[0m i miss my boo so much unhappy\n",
      "  i miss him unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m3\u001b[0m hey thanks for being top new followers this week much appreciated happy want this\n",
      "  hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m4\u001b[0m thanks for the recent follow much appreciated happy want this\n",
      "  thanks for the recent follow much appreciated happy want this for it s magical\n",
      "\n",
      "\u001b[1m\u001b[31m5\u001b[0m thanks for the recent follow much appreciated happy i sent this with\n",
      "  thanks for the recent follow much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m6\u001b[0m thanks for the recent follow happy to connect happy have a great wednesday\n",
      "  thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "\n",
      "\u001b[1m\u001b[31m7\u001b[0m share the love thanks for being top new followers this week happy get it\n",
      "  share the love thanks for being top new followers this week happy\n",
      "\n",
      "\u001b[1m\u001b[31m8\u001b[0m i love them with all my hort unhappy\n",
      "  love it unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m9\u001b[0m thanks for being top engaged community members this week happy want this\n",
      "  thanks for being top engaged community members this week happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m10\u001b[0m have a great thursday looking forward to reading your tweets happy want this\n",
      "  have a great thursday looking forward to reading your tweets happy want this it s free\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Датафреймы для стеммера в мешках слов с обработкой ошибок и нахождение 10 пар похожих твитов\n",
    "df_miss_stemmer_bag, df_miss_stemmer_bag_test, X_train_for_miss_stemmer_bag, y_train_for_miss_stemmer_bag = to_bag(preparation_data['miss_stemmer'], \n",
    "                                                                                                                   preparation_data_test['miss_stemmer'], count_vectorizer)\n",
    "find_ten_pairs(df_miss_stemmer_bag.copy(), X_train_for_miss_stemmer_bag.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0908fd06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m1\u001b[0m hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "  hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "\u001b[1m\u001b[31m2\u001b[0m i miss my boo so much unhappy\n",
      "  i miss him unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m3\u001b[0m hey thanks for being top new followers this week much appreciated happy want this\n",
      "  hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m4\u001b[0m thanks for the recent follow much appreciated happy want this\n",
      "  thanks for the recent follow much appreciated happy want this for it s magical\n",
      "\n",
      "\u001b[1m\u001b[31m5\u001b[0m thanks for the recent follow much appreciated happy i sent this with\n",
      "  thanks for the recent follow much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m6\u001b[0m thanks for the recent follow happy to connect happy have a great wednesday\n",
      "  thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "\n",
      "\u001b[1m\u001b[31m7\u001b[0m share the love thanks for being top new followers this week happy get it\n",
      "  share the love thanks for being top new followers this week happy\n",
      "\n",
      "\u001b[1m\u001b[31m8\u001b[0m i love them with all my hort unhappy\n",
      "  love it unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m9\u001b[0m thanks for being top engaged community members this week happy want this\n",
      "  thanks for being top engaged community members this week happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m10\u001b[0m have a great thursday looking forward to reading your tweets happy want this\n",
      "  have a great thursday looking forward to reading your tweets happy want this it s free\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Датафреймы для леммы в мешках слов с обработкой ошибок и нахождение 10 пар похожих твитов\n",
    "df_miss_lemma_bag, df_miss_lemma_bag_test, X_train_for_miss_lemma_bag, y_train_for_miss_lemma_bag = to_bag(preparation_data['miss_lemma'], \n",
    "                                                                                                           preparation_data_test['miss_lemma'], count_vectorizer)\n",
    "find_ten_pairs(df_miss_lemma_bag.copy(), X_train_for_miss_lemma_bag.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b286b6eb",
   "metadata": {},
   "source": [
    "## Создаём бинарные мешки слов со стоп-словами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76c56211",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_bool = CountVectorizer(binary=True, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6734b92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m1\u001b[0m i miss him unhappy\n",
      "  where s justin i miss him unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m2\u001b[0m hey thanks for being top new followers this week much appreciated happy want this\n",
      "  hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m3\u001b[0m hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "  hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "\u001b[1m\u001b[31m4\u001b[0m share the love thanks for being top new followers this week happy get it\n",
      "  share the love thanks for being top new followers this week happy\n",
      "\n",
      "\u001b[1m\u001b[31m5\u001b[0m thanks for being top engaged community members this week happy want this\n",
      "  thanks for being top engaged community members this week happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m6\u001b[0m thanks for the recent follow much appreciated happy want this\n",
      "  thanks for the recent follow much appreciated happy want this for it s magical\n",
      "\n",
      "\u001b[1m\u001b[31m7\u001b[0m thanks for the recent follow happy to connect happy have a great wednesday\n",
      "  thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "\n",
      "\u001b[1m\u001b[31m8\u001b[0m i love them with all my hort unhappy\n",
      "  love it unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m9\u001b[0m thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "  thanks for the recent follow happy to connect happy have a great thursday want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m10\u001b[0m have a great thursday looking forward to reading your tweets happy want this\n",
      "  have a great thursday looking forward to reading your tweets happy want this it s free\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Датафреймы для токенов в бинарных мешках слов и нахождение 10 пар похожих твитов\n",
    "df_token_bag_binary, df_token_bag_binary_test, X_train_for_token_bag_binary, y_train_for_token_bag_binary = to_bag(preparation_data['token'], \n",
    "                                                                                                                   preparation_data_test['token'], count_vectorizer_bool)\n",
    "find_ten_pairs(df_token_bag_binary.copy(), X_train_for_token_bag_binary.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa254d23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m1\u001b[0m hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "  hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "\u001b[1m\u001b[31m2\u001b[0m hey thanks for being top new followers this week much appreciated happy want this\n",
      "  hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m3\u001b[0m i miss him unhappy\n",
      "  where s justin i miss him unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m4\u001b[0m share the love thanks for being top new followers this week happy get it\n",
      "  share the love thanks for being top new followers this week happy\n",
      "\n",
      "\u001b[1m\u001b[31m5\u001b[0m thanks for the recent follow much appreciated happy want this\n",
      "  thanks for the recent follow much appreciated happy want this for it s magical\n",
      "\n",
      "\u001b[1m\u001b[31m6\u001b[0m thanks for the recent follow much appreciated happy i sent this with\n",
      "  for the recent follow much appreciated happy want this\n",
      "\n",
      "\u001b[1m\u001b[31m7\u001b[0m thanks for being top engaged community members this week happy want this\n",
      "  thanks for being top engaged community members this week happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m8\u001b[0m thanks for the recent follow much appreciated happy get it\n",
      "  thanks for the recent follow much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m9\u001b[0m have a great thursday looking forward to reading your tweets happy want this\n",
      "  have a great thursday looking forward to reading your tweets happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m10\u001b[0m thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "  thanks for the recent follow happy to connect happy have a great thursday want this it s free\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Датафреймы для стеммера в бинарных мешках слов и нахождение 10 пар похожих твитов\n",
    "df_stemmer_bag_binary, df_stemmer_bag_binary_test, X_train_for_stemmer_bag_binary, y_train_for_stemmer_bag_binary = to_bag(preparation_data['stemmer'], \n",
    "                                                                                                                           preparation_data_test['stemmer'], count_vectorizer_bool)\n",
    "find_ten_pairs(df_stemmer_bag_binary.copy(), X_train_for_stemmer_bag_binary.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f70de5c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m1\u001b[0m i miss him unhappy\n",
      "  where s justin i miss him unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m2\u001b[0m hey thanks for being top new followers this week much appreciated happy want this\n",
      "  hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m3\u001b[0m hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "  hi ashish we tried to call your number but got no response unhappy please share another suitable time and an alternate cont\n",
      "\n",
      "\u001b[1m\u001b[31m4\u001b[0m share the love thanks for being top new followers this week happy get it\n",
      "  share the love thanks for being top new followers this week happy\n",
      "\n",
      "\u001b[1m\u001b[31m5\u001b[0m thanks for being top engaged community members this week happy want this\n",
      "  thanks for being top engaged community members this week happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m6\u001b[0m rt good evening everyone happy join our twitter party tonight official tagline tanner welcomebackph tmi\n",
      "  rt almightytanner good evening everyone happy join our twitter party tonight official tagline tanner welcomeb\n",
      "\n",
      "\u001b[1m\u001b[31m7\u001b[0m thanks for the recent follow much appreciated happy want this\n",
      "  thanks for the recent follow much appreciated happy want this for it s magical\n",
      "\n",
      "\u001b[1m\u001b[31m8\u001b[0m stats for the week have arrived new follower and no unfollowers happy via\n",
      "  stats for the day have arrived to new followers and no unfollowers happy via\n",
      "\n",
      "\u001b[1m\u001b[31m9\u001b[0m thanks for the recent follow much appreciated happy i sent this with\n",
      "  thanks for the recent follow much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m10\u001b[0m i love them with all my hort unhappy\n",
      "  love it unhappy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Датафреймы для леммы в бинарных мешках слов и нахождение 10 пар похожих твитов\n",
    "df_lemma_bag_binary, df_lemma_bag_binary_test, X_train_for_lemma_bag_binary, y_train_for_lemma_bag_binary = to_bag(preparation_data['lemma'], \n",
    "                                                                                                                   preparation_data_test['lemma'], count_vectorizer_bool)\n",
    "find_ten_pairs(df_lemma_bag_binary.copy(), X_train_for_lemma_bag_binary.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb022593",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m1\u001b[0m i miss my boo so much unhappy\n",
      "  i miss him unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m2\u001b[0m hey thanks for being top new followers this week much appreciated happy want this\n",
      "  hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m3\u001b[0m hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "  hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "\u001b[1m\u001b[31m4\u001b[0m thanks for the recent follow much appreciated happy want this\n",
      "  thanks for the recent follow much appreciated happy want this for it s magical\n",
      "\n",
      "\u001b[1m\u001b[31m5\u001b[0m share the love thanks for being top new followers this week happy get it\n",
      "  share the love thanks for being top new followers this week happy\n",
      "\n",
      "\u001b[1m\u001b[31m6\u001b[0m thanks for the recent follow much appreciated happy i sent this with\n",
      "  thanks for the recent follow much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m7\u001b[0m please unhappy\n",
      "  please read unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m8\u001b[0m thanks for being top engaged community members this week happy want this\n",
      "  thanks for being top engaged community members this week happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m9\u001b[0m have a great thursday looking forward to reading your tweets happy want this\n",
      "  have a great thursday looking forward to reading your tweets happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m10\u001b[0m rt good evening everyone happy join our twitter party tonight official tagline tanner welcomebackph tmi\n",
      "  rt almightytanner good evening everyone happy join our twitter party tonight official tagline tanner welcomeb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Датафреймы для стеммера в бинарных мешках слов с обработкой ошибок и нахождение 10 пар похожих твитов\n",
    "df_miss_stemmer_bag_binary, df_miss_stemmer_bag_binary_test, X_train_for_miss_stemmer_bag_binary, y_train_for_miss_stemmer_bag_binary = to_bag(preparation_data['miss_stemmer'], \n",
    "                                                                                                                                               preparation_data_test['miss_stemmer'], count_vectorizer_bool)\n",
    "find_ten_pairs(df_miss_stemmer_bag_binary.copy(), X_train_for_miss_stemmer_bag_binary.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04a17165",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m1\u001b[0m i miss my boo so much unhappy\n",
      "  i miss him unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m2\u001b[0m hey thanks for being top new followers this week much appreciated happy want this\n",
      "  hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m3\u001b[0m hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "  hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "\u001b[1m\u001b[31m4\u001b[0m thanks for the recent follow much appreciated happy want this\n",
      "  thanks for the recent follow much appreciated happy want this for it s magical\n",
      "\n",
      "\u001b[1m\u001b[31m5\u001b[0m share the love thanks for being top new followers this week happy get it\n",
      "  share the love thanks for being top new followers this week happy\n",
      "\n",
      "\u001b[1m\u001b[31m6\u001b[0m thanks for the recent follow much appreciated happy i sent this with\n",
      "  thanks for the recent follow much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m7\u001b[0m please unhappy\n",
      "  please read unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m8\u001b[0m thanks for being top engaged community members this week happy want this\n",
      "  thanks for being top engaged community members this week happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m9\u001b[0m have a great thursday looking forward to reading your tweets happy want this\n",
      "  have a great thursday looking forward to reading your tweets happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m10\u001b[0m rt good evening everyone happy join our twitter party tonight official tagline tanner welcomebackph tmi\n",
      "  rt almightytanner good evening everyone happy join our twitter party tonight official tagline tanner welcomeb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Датафреймы для леммы в бинарных мешках слов с обработкой ошибок и нахождение 10 пар похожих твитов\n",
    "df_miss_lemma_bag_binary, df_miss_lemma_bag_binary_test, X_train_for_miss_lemma_bag_binary, y_train_for_miss_lemma_bag_binary = to_bag(preparation_data['miss_lemma'], \n",
    "                                                                                                                                       preparation_data_test['miss_lemma'], count_vectorizer_bool)\n",
    "find_ten_pairs(df_miss_lemma_bag_binary.copy(), X_train_for_miss_lemma_bag_binary.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6757f",
   "metadata": {},
   "source": [
    "## TF-IDF со стоп-словами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "596baa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Скоринг по TF-IDF растет пропорционально частоте появления слова в документе, но это компенсируется количеством документов, содержащих это слово.\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2058363",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m1\u001b[0m hey thanks for being top new followers this week much appreciated happy want this\n",
      "  hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m2\u001b[0m share the love thanks for being top new followers this week happy\n",
      "  share the love thanks for being top new followers this week happy want it\n",
      "\n",
      "\u001b[1m\u001b[31m3\u001b[0m thanks for being top engaged community members this week happy want this\n",
      "  thanks for being top engaged community members this week happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m4\u001b[0m thanks for the recent follow much appreciated happy want this\n",
      "  for the recent follow much appreciated happy want this\n",
      "\n",
      "\u001b[1m\u001b[31m5\u001b[0m thanks for the recent follow happy to connect happy have a great wednesday\n",
      "  thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "\n",
      "\u001b[1m\u001b[31m6\u001b[0m thanks for the recent follow much appreciated happy get it\n",
      "  thanks for the recent follow much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m7\u001b[0m thanks for being top engaged community members this week happy try this too\n",
      "  thanks for being top engaged community members this week happy i sent this with\n",
      "\n",
      "\u001b[1m\u001b[31m8\u001b[0m thanks for the recent follow happy to connect happy have a great thursday get free\n",
      "  thanks for the recent follow happy to connect happy have a great thursday want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m9\u001b[0m hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "  hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "\u001b[1m\u001b[31m10\u001b[0m have a great thursday looking forward to reading your tweets happy want this\n",
      "  have a great thursday looking forward to reading your tweets happy want this it s free\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Датафреймы для токенов TF-IDF и нахождение 10 пар похожих твитов\n",
    "df_token_tfidf, df_token_tfidf_test, X_train_for_token_tfidf, y_train_for_token_tfidf = to_bag(preparation_data['token'], \n",
    "                                                                                               preparation_data_test['token'], tfidf_vectorizer)\n",
    "find_ten_pairs(df_token_tfidf.copy(), X_train_for_token_tfidf.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1fd4aeda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m1\u001b[0m for being top high value members this week happy\n",
      "  thanks for being high value member this week\n",
      "\n",
      "\u001b[1m\u001b[31m2\u001b[0m thanks for being top engaged community members this week happy want this\n",
      "  thanks for being top engaged community members this week happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m3\u001b[0m thanks for being top engaged community members this week happy try this too\n",
      "  thanks for being top engaged community members this week happy i sent this with\n",
      "\n",
      "\u001b[1m\u001b[31m4\u001b[0m thanks for the recent follow happy to connect happy have a great wednesday\n",
      "  thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "\n",
      "\u001b[1m\u001b[31m5\u001b[0m thanks for the recent follow much appreciated happy want this\n",
      "  for the recent follow much appreciated happy want this\n",
      "\n",
      "\u001b[1m\u001b[31m6\u001b[0m much appreciated happy want it\n",
      "  much appreciated happy want this\n",
      "\n",
      "\u001b[1m\u001b[31m7\u001b[0m thanks for the recent follow much appreciated happy get it\n",
      "  thanks for the recent follow much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m8\u001b[0m thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "  thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "\n",
      "\u001b[1m\u001b[31m9\u001b[0m hey thanks for being top new followers this week much appreciated happy want this\n",
      "  hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m10\u001b[0m hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "  hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Датафреймы для стеммера TF-IDF и нахождение 10 пар похожих твитов\n",
    "df_stemmer_tfidf, df_stemmer_tfidf_test, X_train_for_stemmer_tfidf, y_train_for_stemmer_tfidf = to_bag(preparation_data['stemmer'], \n",
    "                                                                                                       preparation_data_test['stemmer'], tfidf_vectorizer)\n",
    "find_ten_pairs(df_stemmer_tfidf.copy(), X_train_for_stemmer_tfidf.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a430ee2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m1\u001b[0m hey thanks for being top new followers this week much appreciated happy want this\n",
      "  hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m2\u001b[0m thanks for the follow happy\n",
      "  thanks for the follow\n",
      "\n",
      "\u001b[1m\u001b[31m3\u001b[0m share the love thanks for being top new followers this week happy\n",
      "  share the love thanks for being top new followers this week happy want it\n",
      "\n",
      "\u001b[1m\u001b[31m4\u001b[0m thanks for the recent follow happy to connect happy have a great wednesday\n",
      "  thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "\n",
      "\u001b[1m\u001b[31m5\u001b[0m thanks for being top engaged community members this week happy want this\n",
      "  thanks for being top engaged community members this week happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m6\u001b[0m thanks for the recent follow happy to connect happy have a great thursday get free\n",
      "  thanks for the recent follow happy to connect happy have a great thursday want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m7\u001b[0m thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "  thanks for the recent follow happy to connect happy have a great thursday\n",
      "\n",
      "\u001b[1m\u001b[31m8\u001b[0m hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "  hi ashish we tried to call your number but got no response unhappy please share another suitable time and an alternate cont\n",
      "\n",
      "\u001b[1m\u001b[31m9\u001b[0m thanks for the recent follow much appreciated happy want this\n",
      "  for the recent follow much appreciated happy want this\n",
      "\n",
      "\u001b[1m\u001b[31m10\u001b[0m thanks for being top engaged community members this week happy try this too\n",
      "  thanks for being top engaged community members this week happy i sent this with\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Датафреймы для леммы TF-IDF и нахождение 10 пар похожих твитов\n",
    "df_lemma_tfidf, df_lemma_tfidf_test, X_train_for_lemma_tfidf, y_train_for_lemma_tfidf = to_bag(preparation_data['lemma'], \n",
    "                                                                                               preparation_data_test['lemma'], tfidf_vectorizer)\n",
    "find_ten_pairs(df_lemma_tfidf.copy(), X_train_for_lemma_tfidf.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3de232a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m1\u001b[0m hey thanks for being top new followers this week much appreciated happy want this\n",
      "  hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m2\u001b[0m share the love thanks for being top new followers this week happy\n",
      "  share the love thanks for being top new followers this week happy want it\n",
      "\n",
      "\u001b[1m\u001b[31m3\u001b[0m thanks for the recent follow much appreciated happy get it\n",
      "  thanks for the recent follow much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m4\u001b[0m thanks for being top engaged community members this week happy want this\n",
      "  thanks for being top engaged community members this week happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m5\u001b[0m thanks for the recent follow much appreciated happy want this\n",
      "  for the recent follow much appreciated happy want this\n",
      "\n",
      "\u001b[1m\u001b[31m6\u001b[0m thanks for the recent follow happy to connect happy have a great wednesday\n",
      "  thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "\n",
      "\u001b[1m\u001b[31m7\u001b[0m thanks for the recent follow happy to connect happy have a great thursday get free\n",
      "  thanks for the recent follow happy to connect happy have a great thursday want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m8\u001b[0m i miss him unhappy\n",
      "  i miss snsd i miss my girls unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m9\u001b[0m thanks for being top engaged community members this week happy try this too\n",
      "  thanks for being top engaged community members this week happy i sent this with\n",
      "\n",
      "\u001b[1m\u001b[31m10\u001b[0m thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "  thanks for the recent follow happy to connect happy have a great thursday\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Датафреймы для стеммера TF-IDF с обработкой ошибок и нахождение 10 пар похожих твитов\n",
    "df_miss_stemmer_tfidf, df_miss_stemmer_tfidf_test, X_train_for_miss_stemmer_tfidf, y_train_for_miss_stemmer_tfidf = to_bag(preparation_data['miss_stemmer'], \n",
    "                                                                                                                           preparation_data_test['miss_stemmer'], tfidf_vectorizer)\n",
    "find_ten_pairs(df_miss_stemmer_tfidf.copy(), X_train_for_miss_stemmer_tfidf.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17396eaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m1\u001b[0m hey thanks for being top new followers this week much appreciated happy want this\n",
      "  hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m2\u001b[0m share the love thanks for being top new followers this week happy\n",
      "  share the love thanks for being top new followers this week happy want it\n",
      "\n",
      "\u001b[1m\u001b[31m3\u001b[0m thanks for the recent follow much appreciated happy get it\n",
      "  thanks for the recent follow much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m4\u001b[0m thanks for being top engaged community members this week happy want this\n",
      "  thanks for being top engaged community members this week happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m5\u001b[0m thanks for the recent follow much appreciated happy want this\n",
      "  for the recent follow much appreciated happy want this\n",
      "\n",
      "\u001b[1m\u001b[31m6\u001b[0m thanks for the recent follow happy to connect happy have a great wednesday\n",
      "  thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "\n",
      "\u001b[1m\u001b[31m7\u001b[0m thanks for the recent follow happy to connect happy have a great thursday get free\n",
      "  thanks for the recent follow happy to connect happy have a great thursday want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m8\u001b[0m i miss him unhappy\n",
      "  i miss snsd i miss my girls unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m9\u001b[0m thanks for being top engaged community members this week happy try this too\n",
      "  thanks for being top engaged community members this week happy i sent this with\n",
      "\n",
      "\u001b[1m\u001b[31m10\u001b[0m thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "  thanks for the recent follow happy to connect happy have a great thursday\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Датафреймы для леммы TF-IDF слов с обработкой ошибок и нахождение 10 пар похожих твитов\n",
    "df_miss_lemma_tfidf, df_miss_lemma_tfidf_test, X_train_for_miss_lemma_tfidf, y_train_for_miss_lemma_tfidf = to_bag(preparation_data['miss_lemma'], \n",
    "                                                                                                                   preparation_data_test['miss_lemma'], tfidf_vectorizer)\n",
    "find_ten_pairs(df_miss_lemma_tfidf.copy(), X_train_for_miss_lemma_tfidf.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa68dc35",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26533fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m1\u001b[0m feel like i shoyould be telling you to get the fuck out social media byout also feel really mean because unhappy silence love yoyou hope yoyoure okay\n",
      "  happy birthday sweet sweet girl i hope you have the best day ever love and miss you so much unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m2\u001b[0m hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "  hi ashish we tried to call your number but got no response unhappy please share another suitable time and an alternate cont\n",
      "\n",
      "\u001b[1m\u001b[31m3\u001b[0m i need a car unhappy but no car unhappy poor me unhappy\n",
      "  who would you call when you are roused in midnightreminds me of the time s they were roommates unhappy unhappy unhappy you suck\n",
      "\n",
      "\u001b[1m\u001b[31m4\u001b[0m thanks so much for following us we d love to know what you think of our flooring range happy\n",
      "  thank you so much sir happy please watch the film and call me happy\n",
      "\n",
      "\u001b[1m\u001b[31m5\u001b[0m hey thanks for being top new followers this week much appreciated happy want this\n",
      "  hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m6\u001b[0m share the love thanks for being top new followers this week happy get it\n",
      "  share the love thanks for being top new followers this week happy want it\n",
      "\n",
      "\u001b[1m\u001b[31m7\u001b[0m hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "  i want to but i wouldn t be able to get time off for it and don t know people in my year to go with unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m8\u001b[0m share the love you re top engaged community members this week much appreciated happy\n",
      "  happy bday midawgdarnizlle i love you a long time happy thanks for being my best friend forever and attempting to travel on the road\n",
      "\n",
      "\u001b[1m\u001b[31m9\u001b[0m thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "  thanks for the recent follow happy to connect happy have a great thursday want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m10\u001b[0m thanks for the recent follow happy to connect happy have a great thursday get free\n",
      "  thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Датафреймы для токенов word2vec и нахождение 10 пар похожих твитов\n",
    "df_token_word2vec, df_token_word2vec_test, X_train_for_token_word2vec, y_train_for_token_word2vec = create_vectors(preparation_data['token'], \n",
    "                                                                                                                   preparation_data_test['token'], 100)\n",
    "find_ten_pairs(df_token_word2vec.copy(), X_train_for_token_word2vec.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b7fa4d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m1\u001b[0m hey thanks for being top new followers this week much appreciated happy want this\n",
      "  share the love thanks for being top new followers this week happy want this\n",
      "\n",
      "\u001b[1m\u001b[31m2\u001b[0m hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "  hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "\u001b[1m\u001b[31m3\u001b[0m share the love thanks for being top new followers this week happy get it\n",
      "  share the love thanks for being top new followers this week happy want it\n",
      "\n",
      "\u001b[1m\u001b[31m4\u001b[0m thanks for the recent follow happy to connect happy have a great thursday get free\n",
      "  thanks for the recent follow happy to connect happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m5\u001b[0m thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "  thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "\n",
      "\u001b[1m\u001b[31m6\u001b[0m thanks for the recent follow happy to connect happy have a great thursday want this it s free\n",
      "  thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "\n",
      "\u001b[1m\u001b[31m7\u001b[0m feel like i shoyould be telling you to get the fuck out social media byout also feel really mean because unhappy silence love yoyou hope yoyoure okay\n",
      "  hello followers thank you so much for following will love to read your tweets happy want this\n",
      "\n",
      "\u001b[1m\u001b[31m8\u001b[0m thanks for being top engaged community members this week happy i sent this with\n",
      "  wtf where s the what is this boring show that s on when did this happen this is what happens when i close all week unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m9\u001b[0m thanks for being top engaged community members this week happy want this\n",
      "  thanks for being top engaged community members this week happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m10\u001b[0m i love when people or message me w long messages about how beautiful they find joon like unhappy yes i agree keep going\n",
      "  happy birthday sweet sweet girl i hope you have the best day ever love and miss you so much unhappy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Датафреймы для стеммера word2vec и нахождение 10 пар похожих твитов\n",
    "df_stemmer_word2vec, df_stemmer_word2vec_test, X_train_for_stemmer_word2vec, y_train_for_stemmer_word2vec = create_vectors(preparation_data['stemmer'], \n",
    "                                                                                                                           preparation_data_test['stemmer'], 100)\n",
    "find_ten_pairs(df_stemmer_word2vec.copy(), X_train_for_stemmer_word2vec.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d6a53455",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m1\u001b[0m hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "  hi ashish we tried to call your number but got no response unhappy please share another suitable time and an alternate cont\n",
      "\n",
      "\u001b[1m\u001b[31m2\u001b[0m i love when people or message me w long messages about how beautiful they find joon like unhappy yes i agree keep going\n",
      "  happy birthday sweet sweet girl i hope you have the best day ever love and miss you so much unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m3\u001b[0m hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "  i hope louis gets all the off time that he wants and comes back fresh and happy happy\n",
      "\n",
      "\u001b[1m\u001b[31m4\u001b[0m share the love thanks for being top new followers this week happy get it\n",
      "  share the love thanks for being top new followers this week happy want it\n",
      "\n",
      "\u001b[1m\u001b[31m5\u001b[0m thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "  thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "\n",
      "\u001b[1m\u001b[31m6\u001b[0m hey thanks for being top new followers this week much appreciated happy want this\n",
      "  hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "\u001b[1m\u001b[31m7\u001b[0m share the love you re top engaged community members this week much appreciated happy\n",
      "  thanks for being top engaged community members this week happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m8\u001b[0m hello followers thank you so much for following will love to read your tweets happy want this\n",
      "  thank you so much sir happy please watch the film and call me happy\n",
      "\n",
      "\u001b[1m\u001b[31m9\u001b[0m this guy got me into exo back in he was also my very first bias in kpop to be honest i miss him so much happy birthd\n",
      "  haha very good tweet mate happy heard your a big boy account the king shit happensould we say happy any chance of a shit happens\n",
      "\n",
      "\u001b[1m\u001b[31m10\u001b[0m thanks for the recent follow happy to connect happy have a great thursday get free\n",
      "  thanks for the recent follow happy to connect happy have a great thursday want this it s free\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Датафреймы для леммы word2vec и нахождение 10 пар похожих твитов\n",
    "df_lemma_word2vec, df_lemma_word2vec_test, X_train_for_lemma_word2vec, y_train_for_lemma_word2vec = create_vectors(preparation_data['lemma'], \n",
    "                                                                                                                   preparation_data_test['lemma'], 100)\n",
    "find_ten_pairs(df_lemma_word2vec.copy(), X_train_for_lemma_word2vec.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d6212932",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m1\u001b[0m feel like i shoyould be telling you to get the fuck out social media byout also feel really mean because unhappy silence love yoyou hope yoyoure okay\n",
      "  happy birthday sweet sweet girl i hope you have the best day ever love and miss you so much unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m2\u001b[0m hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "  hi ashish we tried to call your number but got no response unhappy please share another suitable time and an alternate cont\n",
      "\n",
      "\u001b[1m\u001b[31m3\u001b[0m thanks for the recent follow happy to connect happy have a great thursday\n",
      "  thanks for the recent follow happy to connect happy have a great wednesday\n",
      "\n",
      "\u001b[1m\u001b[31m4\u001b[0m thanks for the recent follow happy to connect happy have a great thursday get free\n",
      "  thanks for the recent follow happy to connect happy have a great thursday want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m5\u001b[0m thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "  thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "\n",
      "\u001b[1m\u001b[31m6\u001b[0m share the love thanks for being top new followers this week happy get it\n",
      "  share the love thanks for being top new followers this week happy want it\n",
      "\n",
      "\u001b[1m\u001b[31m7\u001b[0m i want to but i wouldn t be able to get time off for it and don t know people in my year to go with unhappy\n",
      "  jyoustinbieber can you please follow me on daianeryoufato i ve been trying for too long unhappy i love you so much baby xx april\n",
      "\n",
      "\u001b[1m\u001b[31m8\u001b[0m hey thanks for being top new followers this week much appreciated happy want this\n",
      "  hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "\u001b[1m\u001b[31m9\u001b[0m thanks for being top engaged community members this week happy want this\n",
      "  thanks for being top engaged community members this week happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m10\u001b[0m i hope it doesn t last too long and you at least get to enjoy the weekend and happy birthday again happy\n",
      "  i hope louis gets all the off time that he wants and comes back fresh and happy happy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Датафреймы для стеммера word2vec с обработкой ошибок и нахождение 10 пар похожих твитов\n",
    "df_miss_stemmer_word2vec, df_miss_stemmer_word2vec_test, X_train_for_miss_stemmer_word2vec, y_train_for_miss_stemmer_word2vec = create_vectors(preparation_data['miss_stemmer'], \n",
    "                                                                                                                                               preparation_data_test['miss_stemmer'], 100)\n",
    "find_ten_pairs(df_miss_stemmer_word2vec.copy(), X_train_for_miss_stemmer_word2vec.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b0e00142",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m1\u001b[0m feel like i shoyould be telling you to get the fuck out social media byout also feel really mean because unhappy silence love yoyou hope yoyoure okay\n",
      "  happy birthday sweet sweet girl i hope you have the best day ever love and miss you so much unhappy\n",
      "\n",
      "\u001b[1m\u001b[31m2\u001b[0m hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "  hi ashish we tried to call your number but got no response unhappy please share another suitable time and an alternate cont\n",
      "\n",
      "\u001b[1m\u001b[31m3\u001b[0m thanks for the recent follow happy to connect happy have a great thursday\n",
      "  thanks for the recent follow happy to connect happy have a great wednesday\n",
      "\n",
      "\u001b[1m\u001b[31m4\u001b[0m thanks for the recent follow happy to connect happy have a great thursday get free\n",
      "  thanks for the recent follow happy to connect happy have a great thursday want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m5\u001b[0m thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "  thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "\n",
      "\u001b[1m\u001b[31m6\u001b[0m share the love thanks for being top new followers this week happy get it\n",
      "  share the love thanks for being top new followers this week happy want it\n",
      "\n",
      "\u001b[1m\u001b[31m7\u001b[0m hey thanks for being top new followers this week much appreciated happy want this\n",
      "  hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "\u001b[1m\u001b[31m8\u001b[0m thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "  thanks for the recent follow happy to connect happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m9\u001b[0m thanks for being top engaged community members this week happy want this\n",
      "  thanks for being top engaged community members this week happy want this it s free\n",
      "\n",
      "\u001b[1m\u001b[31m10\u001b[0m ministers of pmln are happy like those students who never studied for exams luckily exams got delayed they ll still ha\n",
      "  haha very good tweet mate happy heard your a big boy account the king shit happensould we say happy any chance of a shit happens\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Датафреймы для леммы word2vec слов с обработкой ошибок и нахождение 10 пар похожих твитов\n",
    "df_miss_lemma_word2vec, df_miss_lemma_word2vec_test, X_train_for_miss_lemma_word2vec, y_train_for_miss_lemma_word2vec = create_vectors(preparation_data['miss_lemma'], \n",
    "                                                                                                                                       preparation_data_test['miss_lemma'], 100)\n",
    "find_ten_pairs(df_miss_lemma_word2vec.copy(), X_train_for_miss_lemma_word2vec.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe4cbde",
   "metadata": {},
   "source": [
    "## Предсказательные модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10e71434",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Создаём списки из полученных ранее датафреймов. Первые для мешков + TF-IDF, вторые для word2vec\n",
    "trains = [df_token_bag, df_stemmer_bag, df_lemma_bag, df_miss_stemmer_bag, df_miss_lemma_bag, df_token_bag_binary, df_stemmer_bag_binary,  df_lemma_bag_binary, \n",
    "          df_miss_stemmer_bag_binary, df_miss_lemma_bag_binary, df_token_tfidf, df_stemmer_tfidf, df_lemma_tfidf, df_miss_stemmer_tfidf, df_miss_lemma_tfidf]\n",
    "tests = [df_token_bag_test, df_stemmer_bag_test, df_lemma_bag_test, df_miss_stemmer_bag_test, df_miss_lemma_bag_test, df_token_bag_binary_test, \n",
    "         df_stemmer_bag_binary_test, df_lemma_bag_binary_test, df_miss_stemmer_bag_binary_test, df_miss_lemma_bag_binary_test, df_token_tfidf_test, \n",
    "         df_stemmer_tfidf_test, df_lemma_tfidf_test, df_miss_stemmer_tfidf_test, df_miss_lemma_tfidf_test]\n",
    "y_y = [y_train_for_token_bag, y_train_for_stemmer_bag, y_train_for_lemma_bag, y_train_for_miss_stemmer_bag, y_train_for_miss_lemma_bag, \n",
    "       y_train_for_token_bag_binary, y_train_for_stemmer_bag_binary, y_train_for_lemma_bag_binary, y_train_for_miss_stemmer_bag_binary, \n",
    "       y_train_for_miss_lemma_bag_binary, y_train_for_token_tfidf, y_train_for_stemmer_tfidf, y_train_for_lemma_tfidf, y_train_for_miss_stemmer_tfidf, \n",
    "       y_train_for_miss_lemma_tfidf]\n",
    "names = ['токены, мешки слов со стоп-словами', 'стеммер, мешки слов со стоп-словами', 'лемма, мешки слов со стоп-словами', \n",
    "         'стеммер без ошибок, мешки слов со стоп-словами', 'лемма без ошибок, мешки слов со стоп-словами', 'токены, бинарные мешки слов со стоп-словами', \n",
    "         'стеммер, бинарные мешки слов со стоп-словами', 'лемма, бинарные мешки слов со стоп-словами', 'стеммер без ошибок, бинарные мешки слов со стоп-словами', \n",
    "         'лемма без ошибок, бинарные мешки слов со стоп-словами', 'токены, tf-idf со стоп-словами', 'стеммер, tf-idf со стоп-словами', \n",
    "         'лемма, tf-idf со стоп-словами', 'стеммер без ошибок, tf-idf со стоп-словами', 'лемма без ошибок, tf-idf со стоп-словами']\n",
    "trains_word2vec = [df_token_word2vec, df_stemmer_word2vec, df_lemma_word2vec, df_miss_stemmer_word2vec, df_miss_lemma_word2vec]\n",
    "tests_word2vec = [df_token_word2vec_test, df_stemmer_word2vec_test, df_lemma_word2vec_test, df_miss_stemmer_word2vec_test, df_miss_lemma_word2vec_test]\n",
    "y_y_word2vec = [y_train_for_token_word2vec, y_train_for_stemmer_word2vec, y_train_for_lemma_word2vec, y_train_for_miss_stemmer_word2vec, \n",
    "                y_train_for_miss_lemma_word2vec]\n",
    "names_word2vec = ['токены, word2vec со стоп-словами', 'стеммер, word2vec со стоп-словами', 'лемма, word2vec со стоп-словами', \n",
    "                  'стеммер без ошибок, word2vec со стоп-словами', 'лемма без ошибок, word2vec со стоп-словами']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd715d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Функция обучения модели предсказания и подсчёта его точности\n",
    "def go_model(model_object, trains, tests, y_y, names):\n",
    "    for i in range(len(trains)):\n",
    "        model = model_object\n",
    "        model.fit(trains[i], y_y[i])\n",
    "        predict = model.predict(tests[i])\n",
    "        print(f'{colored(round(accuracy_score(y_test, predict), 3), \"red\", attrs=[\"bold\"])} \\t {names[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76428657",
   "metadata": {},
   "source": [
    "#### Обучение мешков слов и TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "643074a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m0.885\u001b[0m \t токены, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.883\u001b[0m \t стеммер, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.874\u001b[0m \t лемма, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.883\u001b[0m \t стеммер без ошибок, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.883\u001b[0m \t лемма без ошибок, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.884\u001b[0m \t токены, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.884\u001b[0m \t стеммер, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.874\u001b[0m \t лемма, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.885\u001b[0m \t стеммер без ошибок, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.885\u001b[0m \t лемма без ошибок, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.877\u001b[0m \t токены, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.884\u001b[0m \t стеммер, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.877\u001b[0m \t лемма, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.881\u001b[0m \t стеммер без ошибок, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.881\u001b[0m \t лемма без ошибок, tf-idf со стоп-словами\n"
     ]
    }
   ],
   "source": [
    "go_model(LogisticRegression(random_state = 21, fit_intercept= True, C=1.25), trains, tests, y_y, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2c5ad1ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m0.865\u001b[0m \t токены, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.866\u001b[0m \t стеммер, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.863\u001b[0m \t лемма, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.871\u001b[0m \t стеммер без ошибок, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.871\u001b[0m \t лемма без ошибок, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.866\u001b[0m \t токены, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.866\u001b[0m \t стеммер, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.863\u001b[0m \t лемма, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.87\u001b[0m \t стеммер без ошибок, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.87\u001b[0m \t лемма без ошибок, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.861\u001b[0m \t токены, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.863\u001b[0m \t стеммер, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.866\u001b[0m \t лемма, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.862\u001b[0m \t стеммер без ошибок, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.862\u001b[0m \t лемма без ошибок, tf-idf со стоп-словами\n"
     ]
    }
   ],
   "source": [
    "go_model(DecisionTreeClassifier(max_depth=35, random_state=42), trains, tests, y_y, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "46a5a914",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m0.868\u001b[0m \t токены, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.871\u001b[0m \t стеммер, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.87\u001b[0m \t лемма, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.865\u001b[0m \t стеммер без ошибок, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.865\u001b[0m \t лемма без ошибок, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.859\u001b[0m \t токены, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.868\u001b[0m \t стеммер, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.865\u001b[0m \t лемма, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.865\u001b[0m \t стеммер без ошибок, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.865\u001b[0m \t лемма без ошибок, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.857\u001b[0m \t токены, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.867\u001b[0m \t стеммер, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.858\u001b[0m \t лемма, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.858\u001b[0m \t стеммер без ошибок, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.858\u001b[0m \t лемма без ошибок, tf-idf со стоп-словами\n"
     ]
    }
   ],
   "source": [
    "go_model(RandomForestClassifier(max_depth=40, random_state=42, n_estimators=25, min_samples_split=5), trains, tests, y_y, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "46c1544a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m0.834\u001b[0m \t токены, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.846\u001b[0m \t стеммер, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.787\u001b[0m \t лемма, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.805\u001b[0m \t стеммер без ошибок, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.805\u001b[0m \t лемма без ошибок, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.815\u001b[0m \t токены, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.814\u001b[0m \t стеммер, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.788\u001b[0m \t лемма, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.822\u001b[0m \t стеммер без ошибок, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.822\u001b[0m \t лемма без ошибок, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.761\u001b[0m \t токены, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.805\u001b[0m \t стеммер, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.783\u001b[0m \t лемма, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.754\u001b[0m \t стеммер без ошибок, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.754\u001b[0m \t лемма без ошибок, tf-idf со стоп-словами\n"
     ]
    }
   ],
   "source": [
    "go_model(knn(n_neighbors=13), trains, tests, y_y, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3282e0ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m0.872\u001b[0m \t токены, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.871\u001b[0m \t стеммер, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.865\u001b[0m \t лемма, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.874\u001b[0m \t стеммер без ошибок, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.874\u001b[0m \t лемма без ошибок, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.871\u001b[0m \t токены, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.871\u001b[0m \t стеммер, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.87\u001b[0m \t лемма, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.871\u001b[0m \t стеммер без ошибок, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.871\u001b[0m \t лемма без ошибок, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.883\u001b[0m \t токены, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.876\u001b[0m \t стеммер, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.875\u001b[0m \t лемма, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.881\u001b[0m \t стеммер без ошибок, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.881\u001b[0m \t лемма без ошибок, tf-idf со стоп-словами\n"
     ]
    }
   ],
   "source": [
    "go_model(SVC(kernel='sigmoid', C=1.8, max_iter=500, random_state=0), trains, tests, y_y, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c794314f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m0.866\u001b[0m \t токены, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.859\u001b[0m \t стеммер, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.871\u001b[0m \t лемма, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.861\u001b[0m \t стеммер без ошибок, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.861\u001b[0m \t лемма без ошибок, мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.865\u001b[0m \t токены, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.862\u001b[0m \t стеммер, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.868\u001b[0m \t лемма, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.859\u001b[0m \t стеммер без ошибок, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.859\u001b[0m \t лемма без ошибок, бинарные мешки слов со стоп-словами\n",
      "\u001b[1m\u001b[31m0.857\u001b[0m \t токены, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.861\u001b[0m \t стеммер, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.859\u001b[0m \t лемма, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.859\u001b[0m \t стеммер без ошибок, tf-idf со стоп-словами\n",
      "\u001b[1m\u001b[31m0.859\u001b[0m \t лемма без ошибок, tf-idf со стоп-словами\n"
     ]
    }
   ],
   "source": [
    "go_model(GradientBoostingClassifier(learning_rate=0.5, max_depth=2, random_state=0, tol=0.1), trains, tests, y_y, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06611b48",
   "metadata": {},
   "source": [
    "#### Обучение word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9a8303e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m0.839\u001b[0m \t токены, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.827\u001b[0m \t стеммер, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.825\u001b[0m \t лемма, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.852\u001b[0m \t стеммер без ошибок, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.852\u001b[0m \t лемма без ошибок, word2vec со стоп-словами\n"
     ]
    }
   ],
   "source": [
    "go_model(LogisticRegression(random_state = 21, C=5, solver='newton-cg'), trains_word2vec, tests_word2vec, y_y_word2vec, names_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c558952e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m0.756\u001b[0m \t токены, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.729\u001b[0m \t стеммер, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.707\u001b[0m \t лемма, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.75\u001b[0m \t стеммер без ошибок, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.746\u001b[0m \t лемма без ошибок, word2vec со стоп-словами\n"
     ]
    }
   ],
   "source": [
    "go_model(DecisionTreeClassifier(max_depth=7, random_state=42, min_samples_split=3), trains_word2vec, tests_word2vec, y_y_word2vec, names_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4b9eba3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m0.815\u001b[0m \t токены, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.809\u001b[0m \t стеммер, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.805\u001b[0m \t лемма, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.815\u001b[0m \t стеммер без ошибок, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.822\u001b[0m \t лемма без ошибок, word2vec со стоп-словами\n"
     ]
    }
   ],
   "source": [
    "go_model(RandomForestClassifier(max_depth=18, random_state=42, n_estimators=24, min_samples_split=5), trains_word2vec, tests_word2vec, y_y_word2vec, names_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8fef7670",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m0.825\u001b[0m \t токены, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.778\u001b[0m \t стеммер, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.805\u001b[0m \t лемма, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.835\u001b[0m \t стеммер без ошибок, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.839\u001b[0m \t лемма без ошибок, word2vec со стоп-словами\n"
     ]
    }
   ],
   "source": [
    "go_model(knn(n_neighbors=10, weights='distance', p=5), trains_word2vec, tests_word2vec, y_y_word2vec, names_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c080452a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m0.855\u001b[0m \t токены, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.837\u001b[0m \t стеммер, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.83\u001b[0m \t лемма, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.861\u001b[0m \t стеммер без ошибок, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.863\u001b[0m \t лемма без ошибок, word2vec со стоп-словами\n"
     ]
    }
   ],
   "source": [
    "go_model(SVC(kernel='rbf', C=4.1, max_iter=510, random_state=0), trains_word2vec, tests_word2vec, y_y_word2vec, names_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a44098e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31m0.837\u001b[0m \t токены, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.835\u001b[0m \t стеммер, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.815\u001b[0m \t лемма, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.837\u001b[0m \t стеммер без ошибок, word2vec со стоп-словами\n",
      "\u001b[1m\u001b[31m0.828\u001b[0m \t лемма без ошибок, word2vec со стоп-словами\n"
     ]
    }
   ],
   "source": [
    "go_model(GradientBoostingClassifier(learning_rate=0.5, max_depth=3, random_state=0, tol=0.1), trains_word2vec, tests_word2vec, y_y_word2vec, names_word2vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
